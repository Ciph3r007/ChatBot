{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ciph3r007/ChatBot/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ucKBB-xx9WL",
        "outputId": "731838de-18d8-452d-9df4-7f99de1fc55b"
      },
      "source": [
        "# Install JAX.\n",
        "!pip install --upgrade jax\n",
        "!pip install --upgrade jaxlib\n",
        "!pip install --upgrade trax\n",
        "\n",
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: jax in /usr/local/lib/python3.7/dist-packages (0.2.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already up-to-date: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.65+cuda110)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from jaxlib) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jaxlib) (1.15.0)\n",
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/51/305b839f51d53abb393777f743e497d27bb341478f3fdec4d6ddaccc9fb5/trax-1.3.7-py2.py3-none-any.whl (521kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.12)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/c0/c0fed4301f592c3b56638ae7292612c17d91a43891ba1aaf9636d535beae/tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 10.5MB/s \n",
            "\u001b[?25hCollecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/e4/e2dc66207464795aafecc5c8cef9a35b5c9a61b974ac60a2c306c12bfd4c/t5-0.9.1-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 41.0MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.65+cuda110)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.1.2)\n",
            "Requirement already satisfied, skipping upgrade: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.29.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.4.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.9MB/s \n",
            "\u001b[?25hCollecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 40.3MB/s \n",
            "\u001b[?25hCollecting seqio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/21/8161f170208b4da94036aa74c6974caaeca82a9a634e80bf98e1a0cd6e10/seqio-0.0.3-py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 41.7MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/4b/01fe41784cf0edd8264fbc6fb90750fdaf365441492c425b6ce693c4122f/tfds_nightly-4.2.0.dev202104230108-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 35.4MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from t5->trax) (3.2.5)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: babel in /usr/local/lib/python3.7/dist-packages (from t5->trax) (2.9.0)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/10/37df0bc87ebf84e1414613176340e3aadc3697d2bd112bf63d3d4b1e848a/mesh_tensorflow-0.1.19-py3-none-any.whl (366kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (20.9)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5->trax) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.1.0)\n",
            "Installing collected packages: tensorflow-text, sentencepiece, tokenizers, sacremoses, transformers, tfds-nightly, seqio, portalocker, sacrebleu, rouge-score, mesh-tensorflow, t5, funcsigs, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.19 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 seqio-0.0.3 t5-0.9.1 tensorflow-text-2.4.3 tfds-nightly-4.2.0.dev202104230108 tokenizers-0.10.2 transformers-4.5.1 trax-1.3.7\n",
            "grpc://10.33.239.250:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmf07eJE8p9r"
      },
      "source": [
        "# Chatbot\n",
        "\n",
        "- [1:   Dataset](#1)\n",
        "- [2:   Preprocessing](#2)\n",
        "    - [2.1:   Creating input pipeline](#2.1)\n",
        "- [3:   Model Training](#4)\n",
        "- [4:   Testing](#5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfTPBColehpV"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "# 1. The MultiWoz dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRUiqp2FehpW"
      },
      "source": [
        "Installation and importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfY2FUYMe7UB",
        "outputId": "fb90d1fe-9935-4682-e805-a04376f1961f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVYSzgookQH5",
        "outputId": "265e62d7-8508-4ecd-b7f5-db2714e0ae80"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/colab_data/chatbot/\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/colab_data/chatbot\n",
            "cbot.jpg  model        Reformer.jpg\tReversibleDecoder.png\n",
            "data\t  __pycache__  reversible2.PNG\tw4_unittest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDG-27P6gizD"
      },
      "source": [
        "!pip install -q trax"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV4zpTnSVFIp",
        "outputId": "242806f9-e6e5-46d9-d550-2b177c67fee5"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "!pip list | grep trax"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.7                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBYblWc8YvAN"
      },
      "source": [
        "Dataset INFO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H8pB_yI8p-g",
        "outputId": "00dc32d9-a4a6-4c8a-8a60-4a6312e0b6a2"
      },
      "source": [
        "with open('data/README') as file:\n",
        "    print(file.read())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "#####################################################\n",
            "#  Copyright Cambridge Dialogue Systems Group, 2018 #\n",
            "#####################################################\n",
            "#####################################################\n",
            "\n",
            "Dataset contains the following files:\n",
            "1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have \"MUL\" in their names. Single domain dialogues have either \"SNG\" or \"WOZ\" in their names.\n",
            "2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.\n",
            "3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.\n",
            "4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.\n",
            "5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.\n",
            "6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.\n",
            "7. police_db.json: the Cambridge police station information.\n",
            "8. taxi_db.json: slot-value list for taxi domain.\n",
            "9. valListFile.txt: list of dialogues for validation.\n",
            "10. testListFile.txt: list of dialogues for testing.\n",
            "11. system_acts.json:\n",
            "  There are 6 domains ('Booking', 'Restaurant', 'Hotel', 'Attraction', 'Taxi', 'Train') and 1 dummy domain ('general').\n",
            "  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. 'Hotel-inform' means it is an 'inform' act in the Hotel domain.\n",
            "  Dialogue acts which cannot take slots, e.g., 'good bye', are defined under the 'general' domain.\n",
            "  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.\n",
            "  If a dialogue act takes no slots, e.g., dialogue act 'offer booking' for an utterance 'would you like to take a reservation?', its slot-value pair is ['none', 'none']\n",
            "  There are four types of values:\n",
            "  1) If a slot takes a binary value, e.g., 'has Internet' or 'has park', the value is either 'yes' or 'no'.\n",
            "  2) If a slot is under the act 'request', e.g., 'request' about 'area', the value is expressed as '?'.\n",
            "  3) The value that appears in the utterance e.g., the name of a restaurant.\n",
            "  4) If for some reason the turn does not have an annotation then it is labeled as \"No Annotation.\"\n",
            "12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.\n",
            "13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.\n",
            "14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpB3VlRBehpX"
      },
      "source": [
        "Declaring some CONSTANTS to be used later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnUBmsdbehpX"
      },
      "source": [
        "DATA_FILE = 'data.json'\n",
        "DATA_DIR = './data'\n",
        "DIALOGUE_DB = {}\n",
        "\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "VOCAB_DIR = 'data/vocabs'\n",
        "\n",
        "N_LAYERS = 6\n",
        "CONTINUE = False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yskv4W_ehpX"
      },
      "source": [
        "Loading the MultiWoz dataset from json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K58I5vFB7GlP"
      },
      "source": [
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file: \n",
        "        db = json.load(file)\n",
        "    return db\n",
        "    \n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGBnUfEk8p9x",
        "outputId": "880c267e-4ebe-48c2-8e84-e8ff68d99cda"
      },
      "source": [
        "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of dialogues is: 10438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMdkaGQrehpY"
      },
      "source": [
        "The dialogues are composed of multiple files and the filenames are used as keys in the dictionary. Those with multi-domain dialogues have \"MUL\" in their filenames while single domain dialogues have either \"SNG\" or \"WOZ\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKmwt7_zehpY",
        "outputId": "5e9aef09-eb33-4da4-9af4-a897badf14b4"
      },
      "source": [
        "print(list(DIALOGUE_DB.keys())[0:7]) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SNG01856.json', 'SNG0129.json', 'PMUL1635.json', 'MUL2168.json', 'SNG0073.json', 'SNG01445.json', 'MUL2105.json']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KYeQLnG8p96",
        "outputId": "ffc69a4a-e4f2-4775-f3d0-854e97b4f46f"
      },
      "source": [
        "# get keys of the fifth file in the list above\n",
        "print(DIALOGUE_DB['SNG0073.json'].keys())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['goal', 'log'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-gj5aqF8p9_"
      },
      "source": [
        "Here `goal` points to a dictionary containing several key objectives of the conversation. `log` (a list) on the other hand contains the dialog in each of its item's `text` key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPPWwQ2s8p9_",
        "outputId": "ebc7a2fd-3c84-4109-fa5c-a08ef37edd68"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['goal']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attraction': {},\n",
              " 'hospital': {},\n",
              " 'hotel': {},\n",
              " 'message': [\"You want to book a <span class='emphasis'>taxi</span>. The taxi should go to <span class='emphasis'>pizza hut fen ditton</span> and should depart from <span class='emphasis'>saint john's college</span>\",\n",
              "  \"The taxi should <span class='emphasis'>leave after 17:15</span>\",\n",
              "  \"Make sure you get <span class='emphasis'>car type</span> and <span class='emphasis'>contact number</span>\"],\n",
              " 'police': {},\n",
              " 'restaurant': {},\n",
              " 'taxi': {'fail_info': {},\n",
              "  'info': {'departure': \"saint john's college\",\n",
              "   'destination': 'pizza hut fen ditton',\n",
              "   'leaveAt': '17:15'},\n",
              "  'reqt': ['car type', 'phone']},\n",
              " 'train': {}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq70g950ehpa",
        "outputId": "9dc929da-3547-4443-eb9f-382e74054bf5"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['log'][0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metadata': {},\n",
              " 'text': \"I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Mohcu5ehpa"
      },
      "source": [
        "The conversion goes between two persons back and forth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6JY0sUgehpa",
        "outputId": "f2ddf87c-9861-40e9-9be3-104da0ff80b2"
      },
      "source": [
        "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
        "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1:  I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\n",
            " Person 2:  What time do you want to leave and what time do you want to arrive by?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0BjxciRehpb"
      },
      "source": [
        "def get_conversation(file, data_db):\n",
        "    result = ''\n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    \n",
        "    logs = data_db[file]['log']\n",
        "    \n",
        "    for i in range(len_msg_log):\n",
        "        cur_log = logs[i]['text']\n",
        "        \n",
        "        if i % 2 == 0:\n",
        "            result += delimiter_1\n",
        "        else:\n",
        "            result += delimiter_2\n",
        "            \n",
        "        result += cur_log\n",
        "\n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugvx0noP8p-G",
        "outputId": "1fdd3d00-b2bf-42cf-d6f1-5003274b47d6"
      },
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "\n",
        "print(conversation)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apiFKExehpc"
      },
      "source": [
        "Prettifier function using termcolor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlqWwiriehpc",
        "outputId": "f58836c7-d204-458d-b4d7-74b4fb028224"
      },
      "source": [
        "def print_conversation(conversation):\n",
        "    \n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    split_list_d1 = conversation.split(delimiter_1)\n",
        "    \n",
        "    for sublist in split_list_d1[1:]:\n",
        "        split_list_d2 = sublist.split(delimiter_2)\n",
        "        print(colored(f'Person 1: {split_list_d2[0]}', 'red'))\n",
        "        \n",
        "        if len(split_list_d2) > 1:\n",
        "            print(colored(f'Person 2: {split_list_d2[1]}', 'green'))\n",
        "\n",
        "            \n",
        "print_conversation(conversation)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPerson 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel \u001b[0m\n",
            "\u001b[32mPerson 2: Okay, do you have a specific area you want to stay in? \u001b[0m\n",
            "\u001b[31mPerson 1: no, i just need to make sure it's cheap. oh, and i need parking \u001b[0m\n",
            "\u001b[32mPerson 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? \u001b[0m\n",
            "\u001b[31mPerson 1: Yes, please. 6 people 3 nights starting on tuesday. \u001b[0m\n",
            "\u001b[32mPerson 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? \u001b[0m\n",
            "\u001b[31mPerson 1: how about only 2 nights. \u001b[0m\n",
            "\u001b[32mPerson 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? \u001b[0m\n",
            "\u001b[31mPerson 1: No, that will be all. Good bye. \u001b[0m\n",
            "\u001b[32mPerson 2: Thank you for using our services.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juJWkQI_8p-j"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrnQ9eNV8p-k",
        "outputId": "3f9c48e8-e0db-4781-820e-d3e8ba0b3d2d"
      },
      "source": [
        "all_files = DIALOGUE_DB.keys()\n",
        "untokenized_data = []\n",
        "\n",
        "for file in all_files:\n",
        "    result = get_conversation(file, DIALOGUE_DB)\n",
        "    untokenized_data.append(result)\n",
        "\n",
        "print(untokenized_data[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HYge3h3ehpf"
      },
      "source": [
        "Splitting the list to a train and eval dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buE0b8bjx_p_",
        "outputId": "489770fd-fa14-4a90-c1a9-6291be9cda17"
      },
      "source": [
        "random.shuffle(untokenized_data)\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of conversations in the data set: 10438\n",
            "number of conversations in train set: 9917\n",
            "number of conversations in eval set: 521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47DVWO7Sehpg"
      },
      "source": [
        "<a name=\"2.1\"></a>\n",
        "## Creating input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGUbgsIdehpg"
      },
      "source": [
        "def stream(data):\n",
        "    while True:\n",
        "        d = random.choice(data)\n",
        "        yield (d, d)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS5nbUXzehpg"
      },
      "source": [
        "Let's define our data pipeline for tokenizing and batching our data. We will also filter by maxlen and use bucketing for batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZgK5FAAWwOu"
      },
      "source": [
        "data_pipeline = trax.data.Serial(\n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE),\n",
        "    trax.data.FilterByLength(2048),\n",
        "    trax.data.BucketByLength(boundaries=[128, 256, 512, 1024],\n",
        "                             batch_sizes=[256, 128, 64, 32, 16]),\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwFEbYuiYqbo"
      },
      "source": [
        "Peek into the train stream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iBQEvhLYRot",
        "outputId": "09660836-29ed-4a13-9b50-d1f0cee9886f"
      },
      "source": [
        "# the stream generators will yield (input, target, mask_weights).\n",
        "inp, _, _ = next(train_stream)\n",
        "print(\"input shape: \", inp.shape)\n",
        "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape:  (16, 512)\n",
            " Person 1: I'm looking for a restaurant in the cheap price range and in the north side of town. Person 2: We have an Italian and an Indian restaurant that are both inexpensive in the north part of town. Would you like the addresses of those? Person 1: I am really looking for  italian food.  Person 2: Well here is Da Vinci Pizzeria in the north. It's cheap. 20 Milton Road Chesterton.  Person 1: Yes, that would be fine. Can you book a table for 4 at 16:00 on Wednesday? Person 2: Absolutely. You're booked for 16:00 and the reference number is I3WAD456. Can I help you with anything else today? Person 1: Great! Yes, I'll also need to find a hotel with free parking and free wifi.  Person 2: I would recommend the ashley hotel.  Person 1: Actually, I apologize for not being more specific, I want to stay in a guesthouse Person 2: Is there a price range or area you prefer? Person 1: Yes I am looking for hotel that  is cheap have free wifi and free parking. Person 2: There are 9 cheaply priced guesthouses with free parking and Wifi. Is there are certain area of the city you'd like to stay in? Person 1: No, as long as it has free wifi and parking. Person 2: Alexander bed and breakfast is in the cheap price range and has both free parking and wifi, would you like a reservation? Person 1: No, just the address, area please. Person 2: alexander bed and breakfast address is 56 saint barnabas road in centre.  Person 1: Great, thanks for the help! Person 2: Glad I could help. Do you need assistance with anything else today? Person 1: What area is that in Person 2: It is located in the city centre, anything else? Person 1: That is all. Thanks for the help! Bye! Person 2: I'm happy to be of service, and I hope you enjoy your stay in Cambridge!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsfaBEyd4Ks4"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "# 3. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RidbAcoR6duP"
      },
      "source": [
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "    model = trax.models.reformer.ReformerLM(\n",
        "        vocab_size=vocab_size,\n",
        "        n_layers=n_layers,\n",
        "        mode=mode,\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKRTLXAnehpi",
        "outputId": "34173016-3211-4279-865b-8903dd417336"
      },
      "source": [
        "temp_model = ReformerLM(mode='train')\n",
        "print(str(temp_model))\n",
        "\n",
        "del temp_model "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_33000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Serial[\n",
            "    Dense_33000\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQehGhoD4Psl"
      },
      "source": [
        "def training_loop(ReformerLM, train_gen, eval_gen, n_layers=2, output_dir = \"./model/\"):\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
        "    \n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_gen,\n",
        "        loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        lr_schedule=lr_schedule,\n",
        "        n_steps_per_checkpoint=50\n",
        "    )\n",
        "    \n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=eval_gen,\n",
        "        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()]\n",
        "    )\n",
        "    \n",
        "    loop = training.Loop(model=ReformerLM(n_layers=n_layers),\n",
        "                         tasks=[train_task],\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    \n",
        "    return loop"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdE7ie71aXxl"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9jERXY46I6J",
        "outputId": "702f007e-0deb-437b-9d00-79adbd772167"
      },
      "source": [
        "if CONTINUE == False:\n",
        "  !rm -f model/model.pkl.gz\n",
        "  loop = training_loop(ReformerLM, train_stream, eval_stream, n_layers=N_LAYERS)\n",
        "else:\n",
        "  loop = training_loop(ReformerLM, train_stream, eval_stream, n_layers=N_LAYERS)\n",
        "  looo.model.init_from_file('model/model.pkl.gz')\n",
        "\n",
        "loop.run(500)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 70673640\n",
            "Step      1: Ran 1 train steps in 102.01 secs\n",
            "Step      1: train WeightedCategoryCrossEntropy |  10.45131302\n",
            "Step      1: eval  WeightedCategoryCrossEntropy |  10.41211796\n",
            "Step      1: eval      WeightedCategoryAccuracy |  0.00000000\n",
            "\n",
            "Step     50: Ran 49 train steps in 460.27 secs\n",
            "Step     50: train WeightedCategoryCrossEntropy |  7.28086758\n",
            "Step     50: eval  WeightedCategoryCrossEntropy |  5.59775305\n",
            "Step     50: eval      WeightedCategoryAccuracy |  0.06433684\n",
            "\n",
            "Step    100: Ran 50 train steps in 282.23 secs\n",
            "Step    100: train WeightedCategoryCrossEntropy |  5.60254526\n",
            "Step    100: eval  WeightedCategoryCrossEntropy |  5.57184601\n",
            "Step    100: eval      WeightedCategoryAccuracy |  0.06548415\n",
            "\n",
            "Step    150: Ran 50 train steps in 355.87 secs\n",
            "Step    150: train WeightedCategoryCrossEntropy |  5.46347237\n",
            "Step    150: eval  WeightedCategoryCrossEntropy |  5.05167723\n",
            "Step    150: eval      WeightedCategoryAccuracy |  0.16104890\n",
            "\n",
            "Step    200: Ran 50 train steps in 323.79 secs\n",
            "Step    200: train WeightedCategoryCrossEntropy |  4.82511044\n",
            "Step    200: eval  WeightedCategoryCrossEntropy |  4.24642324\n",
            "Step    200: eval      WeightedCategoryAccuracy |  0.26790637\n",
            "\n",
            "Step    250: Ran 50 train steps in 196.42 secs\n",
            "Step    250: train WeightedCategoryCrossEntropy |  4.05393171\n",
            "Step    250: eval  WeightedCategoryCrossEntropy |  3.73835611\n",
            "Step    250: eval      WeightedCategoryAccuracy |  0.29171646\n",
            "\n",
            "Step    300: Ran 50 train steps in 291.56 secs\n",
            "Step    300: train WeightedCategoryCrossEntropy |  3.66362762\n",
            "Step    300: eval  WeightedCategoryCrossEntropy |  3.59876657\n",
            "Step    300: eval      WeightedCategoryAccuracy |  0.32712954\n",
            "\n",
            "Step    350: Ran 50 train steps in 162.83 secs\n",
            "Step    350: train WeightedCategoryCrossEntropy |  3.41791248\n",
            "Step    350: eval  WeightedCategoryCrossEntropy |  3.41361380\n",
            "Step    350: eval      WeightedCategoryAccuracy |  0.34334576\n",
            "\n",
            "Step    400: Ran 50 train steps in 281.84 secs\n",
            "Step    400: train WeightedCategoryCrossEntropy |  3.28607607\n",
            "Step    400: eval  WeightedCategoryCrossEntropy |  3.24491549\n",
            "Step    400: eval      WeightedCategoryAccuracy |  0.36254877\n",
            "\n",
            "Step    450: Ran 50 train steps in 333.97 secs\n",
            "Step    450: train WeightedCategoryCrossEntropy |  3.20401669\n",
            "Step    450: eval  WeightedCategoryCrossEntropy |  3.32258558\n",
            "Step    450: eval      WeightedCategoryAccuracy |  0.35322887\n",
            "\n",
            "Step    500: Ran 50 train steps in 284.13 secs\n",
            "Step    500: train WeightedCategoryCrossEntropy |  3.17406034\n",
            "Step    500: eval  WeightedCategoryCrossEntropy |  3.22671747\n",
            "Step    500: eval      WeightedCategoryAccuracy |  0.35841137\n",
            "\n",
            "Step    550: Ran 50 train steps in 207.49 secs\n",
            "Step    550: train WeightedCategoryCrossEntropy |  3.17390060\n",
            "Step    550: eval  WeightedCategoryCrossEntropy |  3.32777548\n",
            "Step    550: eval      WeightedCategoryAccuracy |  0.35229170\n",
            "\n",
            "Step    600: Ran 50 train steps in 203.86 secs\n",
            "Step    600: train WeightedCategoryCrossEntropy |  3.15508103\n",
            "Step    600: eval  WeightedCategoryCrossEntropy |  3.00544262\n",
            "Step    600: eval      WeightedCategoryAccuracy |  0.40475807\n",
            "\n",
            "Step    650: Ran 50 train steps in 123.71 secs\n",
            "Step    650: train WeightedCategoryCrossEntropy |  3.17425680\n",
            "Step    650: eval  WeightedCategoryCrossEntropy |  3.14824247\n",
            "Step    650: eval      WeightedCategoryAccuracy |  0.37420720\n",
            "\n",
            "Step    700: Ran 50 train steps in 215.40 secs\n",
            "Step    700: train WeightedCategoryCrossEntropy |  3.23966265\n",
            "Step    700: eval  WeightedCategoryCrossEntropy |  3.24307418\n",
            "Step    700: eval      WeightedCategoryAccuracy |  0.35663605\n",
            "\n",
            "Step    750: Ran 50 train steps in 165.56 secs\n",
            "Step    750: train WeightedCategoryCrossEntropy |  3.18392301\n",
            "Step    750: eval  WeightedCategoryCrossEntropy |  3.29682541\n",
            "Step    750: eval      WeightedCategoryAccuracy |  0.34882703\n",
            "\n",
            "Step    800: Ran 50 train steps in 207.14 secs\n",
            "Step    800: train WeightedCategoryCrossEntropy |  3.27104950\n",
            "Step    800: eval  WeightedCategoryCrossEntropy |  3.38952088\n",
            "Step    800: eval      WeightedCategoryAccuracy |  0.34252131\n",
            "\n",
            "Step    850: Ran 50 train steps in 165.69 secs\n",
            "Step    850: train WeightedCategoryCrossEntropy |  3.32894516\n",
            "Step    850: eval  WeightedCategoryCrossEntropy |  3.33863115\n",
            "Step    850: eval      WeightedCategoryAccuracy |  0.35124439\n",
            "\n",
            "Step    900: Ran 50 train steps in 220.18 secs\n",
            "Step    900: train WeightedCategoryCrossEntropy |  3.41821456\n",
            "Step    900: eval  WeightedCategoryCrossEntropy |  3.49566126\n",
            "Step    900: eval      WeightedCategoryAccuracy |  0.31845677\n",
            "\n",
            "Step    950: Ran 50 train steps in 157.25 secs\n",
            "Step    950: train WeightedCategoryCrossEntropy |  3.35743809\n",
            "Step    950: eval  WeightedCategoryCrossEntropy |  3.21071720\n",
            "Step    950: eval      WeightedCategoryAccuracy |  0.36893871\n",
            "\n",
            "Step   1000: Ran 50 train steps in 248.90 secs\n",
            "Step   1000: train WeightedCategoryCrossEntropy |  3.32016635\n",
            "Step   1000: eval  WeightedCategoryCrossEntropy |  3.34578323\n",
            "Step   1000: eval      WeightedCategoryAccuracy |  0.34544778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAGlkMZW6rKn"
      },
      "source": [
        "<a name=\"4\"></a>\n",
        "# 4. Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s48rWH-Pehpk"
      },
      "source": [
        "def attention(*args, **kwargs):\n",
        "    # number of input positions to remember in a cache when doing fast inference. \n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    # number of input elements to drop once the fast inference input cache fills up.\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    # return the attention layer with the parameters defined above\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# Getting the model with new attention for prediction\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=N_LAYERS,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si_Xo4nPehpk"
      },
      "source": [
        "# TRAX needs the model to be initialized with this shape\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "model.init(shape11)\n",
        "\n",
        "# Loading weights from the trained model\n",
        "model.weights = loop.eval_model.weights\n",
        "\n",
        "# saving the starting state for each new dialogue prediction\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdo9Fziy-8ZW",
        "outputId": "296738b3-595f-4256-ccc0-7b1abb1632e2"
      },
      "source": [
        "str(model) == str(loop.eval_model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB057xZMehpk"
      },
      "source": [
        "Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnBKD1xehpk"
      },
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNw33qRBehpl"
      },
      "source": [
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    input_tokens_with_batch = input_tokens[None]\n",
        "    \n",
        "    # Using the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        model=ReformerLM,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd7Xr4Dmehpm"
      },
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    sentence = ''\n",
        "    counter = 0\n",
        "    \n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break    \n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNZNfs6Zehpm",
        "outputId": "86600569-d53d-4e68-fbf7-157a92e52145"
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: : I would like to book a preference 2: centre of town. \n",
            "Person 1: I't have a preference 1: I have a preference 1: I have a preference 1: I have a preference 2: I's in the centre. \n",
            "Person 2: I would like to book a preference 2: I have a preference 1: I have a preference 2: I have no hotel is in the centre. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTXOnhJEehpm",
        "outputId": "80c8fdb2-471a-49d2-b97c-74f70b58e5b5"
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Is there a hospital nearby?\n",
            "Person 1: : I need to stay. \n",
            "Person 2: I would you have a preference 2: I have a specific day. \n",
            "Person 1: I have a train. \n",
            "Person 1: I have a preference 2: I have a preference 2: I'ely priced restaurant. \n",
            "Person 1: \n",
            "Person 2: I need to book a preference 2: I would like to book a preference 2: I's to book a preference 2: I's in the centre. Person 2. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wor46vEGehpm",
        "outputId": "a978b733-3fea-458b-8a2a-9fd926819acd"
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 1: : I would like to know I have a preference 2: I have a preference 2: I can help you have a preference 2: I would like to book a preference 2: I have a preference 2: I would like to stay? \n",
            "Person 2: I have a preference 2: I have a preference 2: I have a train at 18:45 at 19 ne on Friday \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}