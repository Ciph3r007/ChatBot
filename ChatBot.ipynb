{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ciph3r007/ChatBot/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmf07eJE8p9r"
      },
      "source": [
        "# Chatbot\n",
        "\n",
        "- [1:   Dataset](#1)\n",
        "- [2:   Preprocessing](#2)\n",
        "    - [2.1:   Creating input pipeline](#2.1)\n",
        "- [3:   Model Training](#4)\n",
        "- [4:   Testing](#5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfTPBColehpV"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "# 1. The MultiWoz dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRUiqp2FehpW"
      },
      "source": [
        "Installation and importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfY2FUYMe7UB",
        "outputId": "8010ffea-3cb5-4617-efaf-4361201ba194"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVYSzgookQH5"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/colab_data/chatbot/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDG-27P6gizD",
        "outputId": "793280a8-eabf-454a-a941-817e5dc50f01"
      },
      "source": [
        "!pip install -q trax"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 16.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 41.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 57.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 54.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 38.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 43.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 50.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV4zpTnSVFIp",
        "outputId": "53a735ed-05e9-4aa1-f9d9-7a7427580f7a"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "!pip list | grep trax"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.7                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBYblWc8YvAN"
      },
      "source": [
        "Dataset INFO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H8pB_yI8p-g",
        "outputId": "02b14432-5745-46c9-cc72-6ce86c2a555a"
      },
      "source": [
        "with open('data/README') as file:\n",
        "    print(file.read())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "#####################################################\n",
            "#  Copyright Cambridge Dialogue Systems Group, 2018 #\n",
            "#####################################################\n",
            "#####################################################\n",
            "\n",
            "Dataset contains the following files:\n",
            "1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have \"MUL\" in their names. Single domain dialogues have either \"SNG\" or \"WOZ\" in their names.\n",
            "2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.\n",
            "3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.\n",
            "4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.\n",
            "5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.\n",
            "6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.\n",
            "7. police_db.json: the Cambridge police station information.\n",
            "8. taxi_db.json: slot-value list for taxi domain.\n",
            "9. valListFile.txt: list of dialogues for validation.\n",
            "10. testListFile.txt: list of dialogues for testing.\n",
            "11. system_acts.json:\n",
            "  There are 6 domains ('Booking', 'Restaurant', 'Hotel', 'Attraction', 'Taxi', 'Train') and 1 dummy domain ('general').\n",
            "  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. 'Hotel-inform' means it is an 'inform' act in the Hotel domain.\n",
            "  Dialogue acts which cannot take slots, e.g., 'good bye', are defined under the 'general' domain.\n",
            "  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.\n",
            "  If a dialogue act takes no slots, e.g., dialogue act 'offer booking' for an utterance 'would you like to take a reservation?', its slot-value pair is ['none', 'none']\n",
            "  There are four types of values:\n",
            "  1) If a slot takes a binary value, e.g., 'has Internet' or 'has park', the value is either 'yes' or 'no'.\n",
            "  2) If a slot is under the act 'request', e.g., 'request' about 'area', the value is expressed as '?'.\n",
            "  3) The value that appears in the utterance e.g., the name of a restaurant.\n",
            "  4) If for some reason the turn does not have an annotation then it is labeled as \"No Annotation.\"\n",
            "12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.\n",
            "13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.\n",
            "14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpB3VlRBehpX"
      },
      "source": [
        "Declaring some CONSTANTS to be used later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnUBmsdbehpX"
      },
      "source": [
        "DATA_FILE = 'data.json'\n",
        "DATA_DIR = './data'\n",
        "DIALOGUE_DB = {}\n",
        "\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "VOCAB_DIR = 'data/vocabs'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yskv4W_ehpX"
      },
      "source": [
        "Loading the MultiWoz dataset from json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K58I5vFB7GlP"
      },
      "source": [
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file: \n",
        "        db = json.load(file)\n",
        "    return db\n",
        "    \n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGBnUfEk8p9x",
        "outputId": "e1504977-9988-4bf6-8cf3-24bbfd3bbb66"
      },
      "source": [
        "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of dialogues is: 10438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMdkaGQrehpY"
      },
      "source": [
        "The dialogues are composed of multiple files and the filenames are used as keys in the dictionary. Those with multi-domain dialogues have \"MUL\" in their filenames while single domain dialogues have either \"SNG\" or \"WOZ\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKmwt7_zehpY",
        "outputId": "cc32a0be-2b87-4dd6-f83e-74f404796dc4"
      },
      "source": [
        "print(list(DIALOGUE_DB.keys())[0:7]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SNG01856.json', 'SNG0129.json', 'PMUL1635.json', 'MUL2168.json', 'SNG0073.json', 'SNG01445.json', 'MUL2105.json']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KYeQLnG8p96",
        "outputId": "10d2fd47-4220-4c2d-c8c6-4e438be0c49b"
      },
      "source": [
        "# get keys of the fifth file in the list above\n",
        "print(DIALOGUE_DB['SNG0073.json'].keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['goal', 'log'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-gj5aqF8p9_"
      },
      "source": [
        "Here `goal` points to a dictionary containing several key objectives of the conversation. `log` (a list) on the other hand contains the dialog in each of its item's `text` key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPPWwQ2s8p9_",
        "outputId": "4b5a95da-0507-492d-ec5f-3cd3a2aa4d50"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['goal']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attraction': {},\n",
              " 'hospital': {},\n",
              " 'hotel': {},\n",
              " 'message': [\"You want to book a <span class='emphasis'>taxi</span>. The taxi should go to <span class='emphasis'>pizza hut fen ditton</span> and should depart from <span class='emphasis'>saint john's college</span>\",\n",
              "  \"The taxi should <span class='emphasis'>leave after 17:15</span>\",\n",
              "  \"Make sure you get <span class='emphasis'>car type</span> and <span class='emphasis'>contact number</span>\"],\n",
              " 'police': {},\n",
              " 'restaurant': {},\n",
              " 'taxi': {'fail_info': {},\n",
              "  'info': {'departure': \"saint john's college\",\n",
              "   'destination': 'pizza hut fen ditton',\n",
              "   'leaveAt': '17:15'},\n",
              "  'reqt': ['car type', 'phone']},\n",
              " 'train': {}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq70g950ehpa",
        "outputId": "b00598ec-99f6-41fb-9a00-8296c90bc096"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['log'][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metadata': {},\n",
              " 'text': \"I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Mohcu5ehpa"
      },
      "source": [
        "The conversion goes between two persons back and forth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6JY0sUgehpa",
        "outputId": "3bc0194a-1f4c-41f8-8127-31cdf6b773db"
      },
      "source": [
        "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
        "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1:  I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\n",
            " Person 2:  What time do you want to leave and what time do you want to arrive by?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0BjxciRehpb"
      },
      "source": [
        "def get_conversation(file, data_db):\n",
        "    result = ''\n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    \n",
        "    logs = data_db[file]['log']\n",
        "    \n",
        "    for i in range(len_msg_log):\n",
        "        cur_log = logs[i]['text']\n",
        "        \n",
        "        if i % 2 == 0:\n",
        "            result += delimiter_1\n",
        "        else:\n",
        "            result += delimiter_2\n",
        "            \n",
        "        result += cur_log\n",
        "\n",
        "    return result"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugvx0noP8p-G",
        "outputId": "596456ce-2a1f-4975-93be-996e8758fb64"
      },
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "\n",
        "print(conversation)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apiFKExehpc"
      },
      "source": [
        "Prettifier function using termcolor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlqWwiriehpc",
        "outputId": "6e88d32c-3dfb-4166-e451-df8704599677"
      },
      "source": [
        "def print_conversation(conversation):\n",
        "    \n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    split_list_d1 = conversation.split(delimiter_1)\n",
        "    \n",
        "    for sublist in split_list_d1[1:]:\n",
        "        split_list_d2 = sublist.split(delimiter_2)\n",
        "        print(colored(f'Person 1: {split_list_d2[0]}', 'red'))\n",
        "        \n",
        "        if len(split_list_d2) > 1:\n",
        "            print(colored(f'Person 2: {split_list_d2[1]}', 'green'))\n",
        "\n",
        "            \n",
        "print_conversation(conversation)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPerson 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel \u001b[0m\n",
            "\u001b[32mPerson 2: Okay, do you have a specific area you want to stay in? \u001b[0m\n",
            "\u001b[31mPerson 1: no, i just need to make sure it's cheap. oh, and i need parking \u001b[0m\n",
            "\u001b[32mPerson 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? \u001b[0m\n",
            "\u001b[31mPerson 1: Yes, please. 6 people 3 nights starting on tuesday. \u001b[0m\n",
            "\u001b[32mPerson 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? \u001b[0m\n",
            "\u001b[31mPerson 1: how about only 2 nights. \u001b[0m\n",
            "\u001b[32mPerson 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? \u001b[0m\n",
            "\u001b[31mPerson 1: No, that will be all. Good bye. \u001b[0m\n",
            "\u001b[32mPerson 2: Thank you for using our services.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juJWkQI_8p-j"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrnQ9eNV8p-k",
        "outputId": "28d02ad3-064b-4000-c20f-ed1bb20d2f38"
      },
      "source": [
        "all_files = DIALOGUE_DB.keys()\n",
        "untokenized_data = []\n",
        "\n",
        "for file in all_files:\n",
        "    result = get_conversation(file, DIALOGUE_DB)\n",
        "    untokenized_data.append(result)\n",
        "\n",
        "print(untokenized_data[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HYge3h3ehpf"
      },
      "source": [
        "Splitting the list to a train and eval dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buE0b8bjx_p_",
        "outputId": "581ac8ff-9918-4e65-acc0-d12273be9875"
      },
      "source": [
        "random.shuffle(untokenized_data)\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of conversations in the data set: 10438\n",
            "number of conversations in train set: 9917\n",
            "number of conversations in eval set: 521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47DVWO7Sehpg"
      },
      "source": [
        "<a name=\"2.1\"></a>\n",
        "## Creating input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGUbgsIdehpg"
      },
      "source": [
        "def stream(data):\n",
        "    while True:\n",
        "        d = random.choice(data)\n",
        "        yield (d, d)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS5nbUXzehpg"
      },
      "source": [
        "Let's define our data pipeline for tokenizing and batching our data. We will also filter by maxlen and use bucketing for batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZgK5FAAWwOu"
      },
      "source": [
        "data_pipeline = trax.data.Serial(\n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE),\n",
        "    trax.data.FilterByLength(2048),\n",
        "    trax.data.BucketByLength(boundaries=[128, 256, 512, 1024],\n",
        "                             batch_sizes=[16, 8, 4, 2, 1]),\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwFEbYuiYqbo"
      },
      "source": [
        "Peek into the train stream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iBQEvhLYRot",
        "outputId": "f9b43fb7-09c1-417f-b77e-dde28589c427"
      },
      "source": [
        "# the stream generators will yield (input, target, mask_weights).\n",
        "inp, _, _ = next(train_stream)\n",
        "print(\"input shape: \", inp.shape)\n",
        "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape:  (4, 512)\n",
            " Person 1: I am going to Cambridge and need a place to eat that serves lebanese food and is cheaply priced.  Person 2: Unfortunately there are no Lebanese restaurants in the cheap price range. Would you like another cuisine type or price range? Person 1: What kind of cheap restaurants are there in the center of town? Person 2: I'm sorry, there are none. Would you like to change your cuisine type or location? Person 1: how about one that serves indian food? Person 2: I found three cheap indian restaurants in the centre.  Would you like to book at Mahal of Cambridge? Person 1: That sounds great. I need a table for 6 at 16:30 on Saturday, please. And I will need the reference number once it's booked. Person 2: Booking was successful. The table will be reserved for 15 minutes. Your reference number is ZCRT70X4. Can I help with anything else? Person 1: I need to book a train on Sunday as well. Person 2: There are 404 entries for Sunday. Where would you like to depart from? Person 1: I'm departing from cambridge.  Person 2: And what would your preferred destination be? Person 1: The train should arrive in broxbourne by 08:45. Person 2: I have found three trains leaving Cambridge for Broxbourne. They arrive at 06:01, 07:01, and 08:01. Each departs one hour from their arrival time. Which train would you like to book? Person 1: I suppose I will book the train arriving at 7:01. Please provide me with the reference number. Person 2: Sure, I can book that for you. How many tickets would you like? Person 1: I need to make a reservation for the same group of people. May I also have the reference number? Person 2: Excellent. Your total is 85.92 GBP and you may pay that at the station. Your reference number is AE2L0GW1. Is there anything else I can help you with today? Person 1: No, that's all I need. Thank you for all your help today! Person 2: Safe travels, it was a pleasure serving you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsfaBEyd4Ks4"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "# 3. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RidbAcoR6duP"
      },
      "source": [
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "    model = trax.models.reformer.ReformerLM(\n",
        "        vocab_size=vocab_size,\n",
        "        n_layers=n_layers,\n",
        "        mode=mode,\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKRTLXAnehpi",
        "outputId": "868580bf-cc1e-4af8-dbb3-3ad765120bf4"
      },
      "source": [
        "temp_model = ReformerLM(mode='train')\n",
        "print(str(temp_model))\n",
        "\n",
        "del temp_model "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_33000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Serial[\n",
            "    Dense_33000\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQehGhoD4Psl"
      },
      "source": [
        "def training_loop(ReformerLM, train_gen, eval_gen, n_layers=2, output_dir = \"./model/\"):\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
        "    \n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_gen,\n",
        "        loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        lr_schedule=lr_schedule,\n",
        "        n_steps_per_checkpoint=10\n",
        "    )\n",
        "    \n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=eval_gen,\n",
        "        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()]\n",
        "    )\n",
        "    \n",
        "    loop = training.Loop(model=ReformerLM(n_layers=n_layers),\n",
        "                         tasks=[train_task],\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    \n",
        "    return loop"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdE7ie71aXxl"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9jERXY46I6J",
        "outputId": "ed4cd4b7-692e-48e2-ac51-f28d1664d281"
      },
      "source": [
        "!rm -f model/model.pkl.gz\n",
        "loop = training_loop(ReformerLM, train_stream, eval_stream, n_layers=6)\n",
        "loop.run(100)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 70673640\n",
            "Step      1: Ran 1 train steps in 51.31 secs\n",
            "Step      1: train WeightedCategoryCrossEntropy |  10.47020435\n",
            "Step      1: eval  WeightedCategoryCrossEntropy |  10.41814709\n",
            "Step      1: eval      WeightedCategoryAccuracy |  0.00000000\n",
            "\n",
            "Step     10: Ran 9 train steps in 76.01 secs\n",
            "Step     10: train WeightedCategoryCrossEntropy |  10.11940384\n",
            "Step     10: eval  WeightedCategoryCrossEntropy |  9.58206272\n",
            "Step     10: eval      WeightedCategoryAccuracy |  0.07237300\n",
            "\n",
            "Step     20: Ran 10 train steps in 38.04 secs\n",
            "Step     20: train WeightedCategoryCrossEntropy |  8.92206955\n",
            "Step     20: eval  WeightedCategoryCrossEntropy |  7.84104586\n",
            "Step     20: eval      WeightedCategoryAccuracy |  0.06186317\n",
            "\n",
            "Step     30: Ran 10 train steps in 38.30 secs\n",
            "Step     30: train WeightedCategoryCrossEntropy |  6.95686340\n",
            "Step     30: eval  WeightedCategoryCrossEntropy |  6.06058884\n",
            "Step     30: eval      WeightedCategoryAccuracy |  0.05801622\n",
            "\n",
            "Step     40: Ran 10 train steps in 38.06 secs\n",
            "Step     40: train WeightedCategoryCrossEntropy |  5.72558355\n",
            "Step     40: eval  WeightedCategoryCrossEntropy |  5.67571354\n",
            "Step     40: eval      WeightedCategoryAccuracy |  0.04839858\n",
            "\n",
            "Step     50: Ran 10 train steps in 55.48 secs\n",
            "Step     50: train WeightedCategoryCrossEntropy |  5.68830919\n",
            "Step     50: eval  WeightedCategoryCrossEntropy |  5.58275938\n",
            "Step     50: eval      WeightedCategoryAccuracy |  0.06600863\n",
            "\n",
            "Step     60: Ran 10 train steps in 41.00 secs\n",
            "Step     60: train WeightedCategoryCrossEntropy |  5.62725306\n",
            "Step     60: eval  WeightedCategoryCrossEntropy |  5.69154358\n",
            "Step     60: eval      WeightedCategoryAccuracy |  0.05653021\n",
            "\n",
            "Step     70: Ran 10 train steps in 37.65 secs\n",
            "Step     70: train WeightedCategoryCrossEntropy |  5.65281248\n",
            "Step     70: eval  WeightedCategoryCrossEntropy |  5.64666986\n",
            "Step     70: eval      WeightedCategoryAccuracy |  0.06654836\n",
            "\n",
            "Step     80: Ran 10 train steps in 38.06 secs\n",
            "Step     80: train WeightedCategoryCrossEntropy |  5.70097208\n",
            "Step     80: eval  WeightedCategoryCrossEntropy |  5.60343218\n",
            "Step     80: eval      WeightedCategoryAccuracy |  0.06408706\n",
            "\n",
            "Step     90: Ran 10 train steps in 38.75 secs\n",
            "Step     90: train WeightedCategoryCrossEntropy |  5.65484571\n",
            "Step     90: eval  WeightedCategoryCrossEntropy |  5.59137917\n",
            "Step     90: eval      WeightedCategoryAccuracy |  0.06043956\n",
            "\n",
            "Step    100: Ran 10 train steps in 37.78 secs\n",
            "Step    100: train WeightedCategoryCrossEntropy |  5.61840391\n",
            "Step    100: eval  WeightedCategoryCrossEntropy |  5.61083794\n",
            "Step    100: eval      WeightedCategoryAccuracy |  0.05617977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTnA9OnV_kWA",
        "outputId": "d6898981-5d4e-4400-c74f-7ea55f766cbf"
      },
      "source": [
        "loop.run(1000)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step    110: Ran 10 train steps in 39.01 secs\n",
            "Step    110: train WeightedCategoryCrossEntropy |  5.58990049\n",
            "Step    110: eval  WeightedCategoryCrossEntropy |  5.67875767\n",
            "Step    110: eval      WeightedCategoryAccuracy |  0.06654344\n",
            "\n",
            "Step    120: Ran 10 train steps in 39.58 secs\n",
            "Step    120: train WeightedCategoryCrossEntropy |  5.62603760\n",
            "Step    120: eval  WeightedCategoryCrossEntropy |  5.56408739\n",
            "Step    120: eval      WeightedCategoryAccuracy |  0.06999353\n",
            "\n",
            "Step    130: Ran 10 train steps in 39.96 secs\n",
            "Step    130: train WeightedCategoryCrossEntropy |  5.61853743\n",
            "Step    130: eval  WeightedCategoryCrossEntropy |  5.48290586\n",
            "Step    130: eval      WeightedCategoryAccuracy |  0.06928105\n",
            "\n",
            "Step    140: Ran 10 train steps in 39.63 secs\n",
            "Step    140: train WeightedCategoryCrossEntropy |  5.58797169\n",
            "Step    140: eval  WeightedCategoryCrossEntropy |  5.58695459\n",
            "Step    140: eval      WeightedCategoryAccuracy |  0.07304461\n",
            "\n",
            "Step    150: Ran 10 train steps in 40.01 secs\n",
            "Step    150: train WeightedCategoryCrossEntropy |  5.61261892\n",
            "Step    150: eval  WeightedCategoryCrossEntropy |  5.72259760\n",
            "Step    150: eval      WeightedCategoryAccuracy |  0.07317074\n",
            "\n",
            "Step    160: Ran 10 train steps in 40.04 secs\n",
            "Step    160: train WeightedCategoryCrossEntropy |  5.65810537\n",
            "Step    160: eval  WeightedCategoryCrossEntropy |  5.46971893\n",
            "Step    160: eval      WeightedCategoryAccuracy |  0.07975460\n",
            "\n",
            "Step    170: Ran 10 train steps in 39.79 secs\n",
            "Step    170: train WeightedCategoryCrossEntropy |  5.60769081\n",
            "Step    170: eval  WeightedCategoryCrossEntropy |  5.50683594\n",
            "Step    170: eval      WeightedCategoryAccuracy |  0.09870317\n",
            "\n",
            "Step    180: Ran 10 train steps in 39.83 secs\n",
            "Step    180: train WeightedCategoryCrossEntropy |  5.52023602\n",
            "Step    180: eval  WeightedCategoryCrossEntropy |  5.53078699\n",
            "Step    180: eval      WeightedCategoryAccuracy |  0.09727082\n",
            "\n",
            "Step    190: Ran 10 train steps in 39.98 secs\n",
            "Step    190: train WeightedCategoryCrossEntropy |  5.41480589\n",
            "Step    190: eval  WeightedCategoryCrossEntropy |  5.23756123\n",
            "Step    190: eval      WeightedCategoryAccuracy |  0.13387297\n",
            "\n",
            "Step    200: Ran 10 train steps in 39.81 secs\n",
            "Step    200: train WeightedCategoryCrossEntropy |  5.24197912\n",
            "Step    200: eval  WeightedCategoryCrossEntropy |  5.34990692\n",
            "Step    200: eval      WeightedCategoryAccuracy |  0.12568305\n",
            "\n",
            "Step    210: Ran 10 train steps in 39.97 secs\n",
            "Step    210: train WeightedCategoryCrossEntropy |  5.13217878\n",
            "Step    210: eval  WeightedCategoryCrossEntropy |  5.15349293\n",
            "Step    210: eval      WeightedCategoryAccuracy |  0.14556482\n",
            "\n",
            "Step    220: Ran 10 train steps in 39.94 secs\n",
            "Step    220: train WeightedCategoryCrossEntropy |  5.03313780\n",
            "Step    220: eval  WeightedCategoryCrossEntropy |  4.87724113\n",
            "Step    220: eval      WeightedCategoryAccuracy |  0.19047619\n",
            "\n",
            "Step    230: Ran 10 train steps in 39.78 secs\n",
            "Step    230: train WeightedCategoryCrossEntropy |  4.93804932\n",
            "Step    230: eval  WeightedCategoryCrossEntropy |  4.88909769\n",
            "Step    230: eval      WeightedCategoryAccuracy |  0.17662860\n",
            "\n",
            "Step    240: Ran 10 train steps in 40.06 secs\n",
            "Step    240: train WeightedCategoryCrossEntropy |  4.91051435\n",
            "Step    240: eval  WeightedCategoryCrossEntropy |  4.60631084\n",
            "Step    240: eval      WeightedCategoryAccuracy |  0.24460874\n",
            "\n",
            "Step    250: Ran 10 train steps in 40.10 secs\n",
            "Step    250: train WeightedCategoryCrossEntropy |  4.76817608\n",
            "Step    250: eval  WeightedCategoryCrossEntropy |  4.54649925\n",
            "Step    250: eval      WeightedCategoryAccuracy |  0.22621566\n",
            "\n",
            "Step    260: Ran 10 train steps in 39.97 secs\n",
            "Step    260: train WeightedCategoryCrossEntropy |  4.69914484\n",
            "Step    260: eval  WeightedCategoryCrossEntropy |  4.55914021\n",
            "Step    260: eval      WeightedCategoryAccuracy |  0.23242559\n",
            "\n",
            "Step    270: Ran 10 train steps in 63.80 secs\n",
            "Step    270: train WeightedCategoryCrossEntropy |  4.64422035\n",
            "Step    270: eval  WeightedCategoryCrossEntropy |  4.66847897\n",
            "Step    270: eval      WeightedCategoryAccuracy |  0.19832404\n",
            "\n",
            "Step    280: Ran 10 train steps in 39.13 secs\n",
            "Step    280: train WeightedCategoryCrossEntropy |  4.60031462\n",
            "Step    280: eval  WeightedCategoryCrossEntropy |  4.55174971\n",
            "Step    280: eval      WeightedCategoryAccuracy |  0.24412172\n",
            "\n",
            "Step    290: Ran 10 train steps in 39.20 secs\n",
            "Step    290: train WeightedCategoryCrossEntropy |  4.45588970\n",
            "Step    290: eval  WeightedCategoryCrossEntropy |  4.33883047\n",
            "Step    290: eval      WeightedCategoryAccuracy |  0.22306630\n",
            "\n",
            "Step    300: Ran 10 train steps in 44.52 secs\n",
            "Step    300: train WeightedCategoryCrossEntropy |  4.44007921\n",
            "Step    300: eval  WeightedCategoryCrossEntropy |  4.35953426\n",
            "Step    300: eval      WeightedCategoryAccuracy |  0.24285714\n",
            "\n",
            "Step    310: Ran 10 train steps in 39.05 secs\n",
            "Step    310: train WeightedCategoryCrossEntropy |  4.42593765\n",
            "Step    310: eval  WeightedCategoryCrossEntropy |  4.21134520\n",
            "Step    310: eval      WeightedCategoryAccuracy |  0.22800258\n",
            "\n",
            "Step    320: Ran 10 train steps in 38.82 secs\n",
            "Step    320: train WeightedCategoryCrossEntropy |  4.30981731\n",
            "Step    320: eval  WeightedCategoryCrossEntropy |  4.21817493\n",
            "Step    320: eval      WeightedCategoryAccuracy |  0.23902439\n",
            "\n",
            "Step    330: Ran 10 train steps in 40.23 secs\n",
            "Step    330: train WeightedCategoryCrossEntropy |  4.40925169\n",
            "Step    330: eval  WeightedCategoryCrossEntropy |  4.27448606\n",
            "Step    330: eval      WeightedCategoryAccuracy |  0.26719198\n",
            "\n",
            "Step    340: Ran 10 train steps in 38.86 secs\n",
            "Step    340: train WeightedCategoryCrossEntropy |  4.38439560\n",
            "Step    340: eval  WeightedCategoryCrossEntropy |  4.55837059\n",
            "Step    340: eval      WeightedCategoryAccuracy |  0.22364672\n",
            "\n",
            "Step    350: Ran 10 train steps in 39.10 secs\n",
            "Step    350: train WeightedCategoryCrossEntropy |  4.37324858\n",
            "Step    350: eval  WeightedCategoryCrossEntropy |  4.34496450\n",
            "Step    350: eval      WeightedCategoryAccuracy |  0.21486855\n",
            "\n",
            "Step    360: Ran 10 train steps in 40.72 secs\n",
            "Step    360: train WeightedCategoryCrossEntropy |  4.28646326\n",
            "Step    360: eval  WeightedCategoryCrossEntropy |  4.01926517\n",
            "Step    360: eval      WeightedCategoryAccuracy |  0.27539635\n",
            "\n",
            "Step    370: Ran 10 train steps in 39.32 secs\n",
            "Step    370: train WeightedCategoryCrossEntropy |  4.30279922\n",
            "Step    370: eval  WeightedCategoryCrossEntropy |  4.24091005\n",
            "Step    370: eval      WeightedCategoryAccuracy |  0.24150711\n",
            "\n",
            "Step    380: Ran 10 train steps in 39.13 secs\n",
            "Step    380: train WeightedCategoryCrossEntropy |  4.22016859\n",
            "Step    380: eval  WeightedCategoryCrossEntropy |  4.22114706\n",
            "Step    380: eval      WeightedCategoryAccuracy |  0.24453281\n",
            "\n",
            "Step    390: Ran 10 train steps in 38.72 secs\n",
            "Step    390: train WeightedCategoryCrossEntropy |  4.13987827\n",
            "Step    390: eval  WeightedCategoryCrossEntropy |  4.13668346\n",
            "Step    390: eval      WeightedCategoryAccuracy |  0.23346056\n",
            "\n",
            "Step    400: Ran 10 train steps in 39.81 secs\n",
            "Step    400: train WeightedCategoryCrossEntropy |  4.19832134\n",
            "Step    400: eval  WeightedCategoryCrossEntropy |  4.22838020\n",
            "Step    400: eval      WeightedCategoryAccuracy |  0.25052339\n",
            "\n",
            "Step    410: Ran 10 train steps in 39.30 secs\n",
            "Step    410: train WeightedCategoryCrossEntropy |  4.22857523\n",
            "Step    410: eval  WeightedCategoryCrossEntropy |  4.30427742\n",
            "Step    410: eval      WeightedCategoryAccuracy |  0.23729947\n",
            "\n",
            "Step    420: Ran 10 train steps in 39.16 secs\n",
            "Step    420: train WeightedCategoryCrossEntropy |  4.12586355\n",
            "Step    420: eval  WeightedCategoryCrossEntropy |  4.01086950\n",
            "Step    420: eval      WeightedCategoryAccuracy |  0.25000000\n",
            "\n",
            "Step    430: Ran 10 train steps in 40.07 secs\n",
            "Step    430: train WeightedCategoryCrossEntropy |  4.15728712\n",
            "Step    430: eval  WeightedCategoryCrossEntropy |  4.15738106\n",
            "Step    430: eval      WeightedCategoryAccuracy |  0.24484181\n",
            "\n",
            "Step    440: Ran 10 train steps in 38.92 secs\n",
            "Step    440: train WeightedCategoryCrossEntropy |  4.12272358\n",
            "Step    440: eval  WeightedCategoryCrossEntropy |  3.89316010\n",
            "Step    440: eval      WeightedCategoryAccuracy |  0.26804915\n",
            "\n",
            "Step    450: Ran 10 train steps in 38.96 secs\n",
            "Step    450: train WeightedCategoryCrossEntropy |  4.15062714\n",
            "Step    450: eval  WeightedCategoryCrossEntropy |  3.95176077\n",
            "Step    450: eval      WeightedCategoryAccuracy |  0.27431905\n",
            "\n",
            "Step    460: Ran 10 train steps in 39.78 secs\n",
            "Step    460: train WeightedCategoryCrossEntropy |  4.12656975\n",
            "Step    460: eval  WeightedCategoryCrossEntropy |  4.10810900\n",
            "Step    460: eval      WeightedCategoryAccuracy |  0.25398853\n",
            "\n",
            "Step    470: Ran 10 train steps in 38.82 secs\n",
            "Step    470: train WeightedCategoryCrossEntropy |  4.11154175\n",
            "Step    470: eval  WeightedCategoryCrossEntropy |  3.96805787\n",
            "Step    470: eval      WeightedCategoryAccuracy |  0.26978415\n",
            "\n",
            "Step    480: Ran 10 train steps in 38.77 secs\n",
            "Step    480: train WeightedCategoryCrossEntropy |  4.11857939\n",
            "Step    480: eval  WeightedCategoryCrossEntropy |  4.20096636\n",
            "Step    480: eval      WeightedCategoryAccuracy |  0.23636365\n",
            "\n",
            "Step    490: Ran 10 train steps in 39.13 secs\n",
            "Step    490: train WeightedCategoryCrossEntropy |  4.13823605\n",
            "Step    490: eval  WeightedCategoryCrossEntropy |  4.13970232\n",
            "Step    490: eval      WeightedCategoryAccuracy |  0.25538462\n",
            "\n",
            "Step    500: Ran 10 train steps in 39.52 secs\n",
            "Step    500: train WeightedCategoryCrossEntropy |  4.04461479\n",
            "Step    500: eval  WeightedCategoryCrossEntropy |  3.88389468\n",
            "Step    500: eval      WeightedCategoryAccuracy |  0.26493323\n",
            "\n",
            "Step    510: Ran 10 train steps in 38.67 secs\n",
            "Step    510: train WeightedCategoryCrossEntropy |  4.01640368\n",
            "Step    510: eval  WeightedCategoryCrossEntropy |  3.71973872\n",
            "Step    510: eval      WeightedCategoryAccuracy |  0.28519857\n",
            "\n",
            "Step    520: Ran 10 train steps in 38.74 secs\n",
            "Step    520: train WeightedCategoryCrossEntropy |  4.03737640\n",
            "Step    520: eval  WeightedCategoryCrossEntropy |  4.04202604\n",
            "Step    520: eval      WeightedCategoryAccuracy |  0.27190137\n",
            "\n",
            "Step    530: Ran 10 train steps in 40.41 secs\n",
            "Step    530: train WeightedCategoryCrossEntropy |  4.04381895\n",
            "Step    530: eval  WeightedCategoryCrossEntropy |  4.07600546\n",
            "Step    530: eval      WeightedCategoryAccuracy |  0.26777938\n",
            "\n",
            "Step    540: Ran 10 train steps in 38.88 secs\n",
            "Step    540: train WeightedCategoryCrossEntropy |  4.09578276\n",
            "Step    540: eval  WeightedCategoryCrossEntropy |  4.14866066\n",
            "Step    540: eval      WeightedCategoryAccuracy |  0.24504624\n",
            "\n",
            "Step    550: Ran 10 train steps in 38.75 secs\n",
            "Step    550: train WeightedCategoryCrossEntropy |  4.03270197\n",
            "Step    550: eval  WeightedCategoryCrossEntropy |  4.25234270\n",
            "Step    550: eval      WeightedCategoryAccuracy |  0.24821684\n",
            "\n",
            "Step    560: Ran 10 train steps in 40.51 secs\n",
            "Step    560: train WeightedCategoryCrossEntropy |  4.04365540\n",
            "Step    560: eval  WeightedCategoryCrossEntropy |  4.12978029\n",
            "Step    560: eval      WeightedCategoryAccuracy |  0.25431609\n",
            "\n",
            "Step    570: Ran 10 train steps in 39.02 secs\n",
            "Step    570: train WeightedCategoryCrossEntropy |  4.09464025\n",
            "Step    570: eval  WeightedCategoryCrossEntropy |  3.92668462\n",
            "Step    570: eval      WeightedCategoryAccuracy |  0.29564032\n",
            "\n",
            "Step    580: Ran 10 train steps in 39.06 secs\n",
            "Step    580: train WeightedCategoryCrossEntropy |  4.04078627\n",
            "Step    580: eval  WeightedCategoryCrossEntropy |  4.22998810\n",
            "Step    580: eval      WeightedCategoryAccuracy |  0.25439212\n",
            "\n",
            "Step    590: Ran 10 train steps in 39.27 secs\n",
            "Step    590: train WeightedCategoryCrossEntropy |  3.99632263\n",
            "Step    590: eval  WeightedCategoryCrossEntropy |  4.09674740\n",
            "Step    590: eval      WeightedCategoryAccuracy |  0.26651654\n",
            "\n",
            "Step    600: Ran 10 train steps in 39.25 secs\n",
            "Step    600: train WeightedCategoryCrossEntropy |  4.09834862\n",
            "Step    600: eval  WeightedCategoryCrossEntropy |  4.27797890\n",
            "Step    600: eval      WeightedCategoryAccuracy |  0.24215248\n",
            "\n",
            "Step    610: Ran 10 train steps in 38.68 secs\n",
            "Step    610: train WeightedCategoryCrossEntropy |  4.11899137\n",
            "Step    610: eval  WeightedCategoryCrossEntropy |  4.04098463\n",
            "Step    610: eval      WeightedCategoryAccuracy |  0.25460994\n",
            "\n",
            "Step    620: Ran 10 train steps in 38.92 secs\n",
            "Step    620: train WeightedCategoryCrossEntropy |  4.18953705\n",
            "Step    620: eval  WeightedCategoryCrossEntropy |  4.14729452\n",
            "Step    620: eval      WeightedCategoryAccuracy |  0.24326146\n",
            "\n",
            "Step    630: Ran 10 train steps in 39.98 secs\n",
            "Step    630: train WeightedCategoryCrossEntropy |  4.16807985\n",
            "Step    630: eval  WeightedCategoryCrossEntropy |  4.17879200\n",
            "Step    630: eval      WeightedCategoryAccuracy |  0.25487646\n",
            "\n",
            "Step    640: Ran 10 train steps in 38.86 secs\n",
            "Step    640: train WeightedCategoryCrossEntropy |  4.21383810\n",
            "Step    640: eval  WeightedCategoryCrossEntropy |  4.15702820\n",
            "Step    640: eval      WeightedCategoryAccuracy |  0.28562179\n",
            "\n",
            "Step    650: Ran 10 train steps in 38.41 secs\n",
            "Step    650: train WeightedCategoryCrossEntropy |  4.21723843\n",
            "Step    650: eval  WeightedCategoryCrossEntropy |  4.33869076\n",
            "Step    650: eval      WeightedCategoryAccuracy |  0.24165148\n",
            "\n",
            "Step    660: Ran 10 train steps in 40.28 secs\n",
            "Step    660: train WeightedCategoryCrossEntropy |  4.22428989\n",
            "Step    660: eval  WeightedCategoryCrossEntropy |  4.39630413\n",
            "Step    660: eval      WeightedCategoryAccuracy |  0.24290031\n",
            "\n",
            "Step    670: Ran 10 train steps in 38.69 secs\n",
            "Step    670: train WeightedCategoryCrossEntropy |  4.20563841\n",
            "Step    670: eval  WeightedCategoryCrossEntropy |  4.12663984\n",
            "Step    670: eval      WeightedCategoryAccuracy |  0.26610646\n",
            "\n",
            "Step    680: Ran 10 train steps in 38.80 secs\n",
            "Step    680: train WeightedCategoryCrossEntropy |  4.20134878\n",
            "Step    680: eval  WeightedCategoryCrossEntropy |  4.17333078\n",
            "Step    680: eval      WeightedCategoryAccuracy |  0.24848484\n",
            "\n",
            "Step    690: Ran 10 train steps in 40.05 secs\n",
            "Step    690: train WeightedCategoryCrossEntropy |  4.20003176\n",
            "Step    690: eval  WeightedCategoryCrossEntropy |  4.08269024\n",
            "Step    690: eval      WeightedCategoryAccuracy |  0.25376198\n",
            "\n",
            "Step    700: Ran 10 train steps in 38.55 secs\n",
            "Step    700: train WeightedCategoryCrossEntropy |  4.21784735\n",
            "Step    700: eval  WeightedCategoryCrossEntropy |  4.28229427\n",
            "Step    700: eval      WeightedCategoryAccuracy |  0.24152543\n",
            "\n",
            "Step    710: Ran 10 train steps in 38.79 secs\n",
            "Step    710: train WeightedCategoryCrossEntropy |  4.24451399\n",
            "Step    710: eval  WeightedCategoryCrossEntropy |  3.99816656\n",
            "Step    710: eval      WeightedCategoryAccuracy |  0.27389443\n",
            "\n",
            "Step    720: Ran 10 train steps in 38.77 secs\n",
            "Step    720: train WeightedCategoryCrossEntropy |  4.20661592\n",
            "Step    720: eval  WeightedCategoryCrossEntropy |  4.20209599\n",
            "Step    720: eval      WeightedCategoryAccuracy |  0.24014778\n",
            "\n",
            "Step    730: Ran 10 train steps in 40.27 secs\n",
            "Step    730: train WeightedCategoryCrossEntropy |  4.19865847\n",
            "Step    730: eval  WeightedCategoryCrossEntropy |  4.26307535\n",
            "Step    730: eval      WeightedCategoryAccuracy |  0.26497838\n",
            "\n",
            "Step    740: Ran 10 train steps in 39.03 secs\n",
            "Step    740: train WeightedCategoryCrossEntropy |  4.26867485\n",
            "Step    740: eval  WeightedCategoryCrossEntropy |  4.11486483\n",
            "Step    740: eval      WeightedCategoryAccuracy |  0.25563911\n",
            "\n",
            "Step    750: Ran 10 train steps in 39.12 secs\n",
            "Step    750: train WeightedCategoryCrossEntropy |  4.15698004\n",
            "Step    750: eval  WeightedCategoryCrossEntropy |  4.46827936\n",
            "Step    750: eval      WeightedCategoryAccuracy |  0.23345454\n",
            "\n",
            "Step    760: Ran 10 train steps in 40.79 secs\n",
            "Step    760: train WeightedCategoryCrossEntropy |  4.27772856\n",
            "Step    760: eval  WeightedCategoryCrossEntropy |  4.19300413\n",
            "Step    760: eval      WeightedCategoryAccuracy |  0.22019635\n",
            "\n",
            "Step    770: Ran 10 train steps in 38.47 secs\n",
            "Step    770: train WeightedCategoryCrossEntropy |  4.18159246\n",
            "Step    770: eval  WeightedCategoryCrossEntropy |  4.13808966\n",
            "Step    770: eval      WeightedCategoryAccuracy |  0.25075346\n",
            "\n",
            "Step    780: Ran 10 train steps in 38.47 secs\n",
            "Step    780: train WeightedCategoryCrossEntropy |  4.24005079\n",
            "Step    780: eval  WeightedCategoryCrossEntropy |  4.20460510\n",
            "Step    780: eval      WeightedCategoryAccuracy |  0.24425477\n",
            "\n",
            "Step    790: Ran 10 train steps in 39.87 secs\n",
            "Step    790: train WeightedCategoryCrossEntropy |  4.19919682\n",
            "Step    790: eval  WeightedCategoryCrossEntropy |  4.14650488\n",
            "Step    790: eval      WeightedCategoryAccuracy |  0.25112614\n",
            "\n",
            "Step    800: Ran 10 train steps in 38.29 secs\n",
            "Step    800: train WeightedCategoryCrossEntropy |  4.17801285\n",
            "Step    800: eval  WeightedCategoryCrossEntropy |  4.04662228\n",
            "Step    800: eval      WeightedCategoryAccuracy |  0.27246377\n",
            "\n",
            "Step    810: Ran 10 train steps in 38.18 secs\n",
            "Step    810: train WeightedCategoryCrossEntropy |  4.20045900\n",
            "Step    810: eval  WeightedCategoryCrossEntropy |  4.04089785\n",
            "Step    810: eval      WeightedCategoryAccuracy |  0.26302883\n",
            "\n",
            "Step    820: Ran 10 train steps in 38.38 secs\n",
            "Step    820: train WeightedCategoryCrossEntropy |  4.11410427\n",
            "Step    820: eval  WeightedCategoryCrossEntropy |  4.11041403\n",
            "Step    820: eval      WeightedCategoryAccuracy |  0.25394547\n",
            "\n",
            "Step    830: Ran 10 train steps in 39.98 secs\n",
            "Step    830: train WeightedCategoryCrossEntropy |  4.11218882\n",
            "Step    830: eval  WeightedCategoryCrossEntropy |  4.30464649\n",
            "Step    830: eval      WeightedCategoryAccuracy |  0.25580010\n",
            "\n",
            "Step    840: Ran 10 train steps in 38.96 secs\n",
            "Step    840: train WeightedCategoryCrossEntropy |  4.10955763\n",
            "Step    840: eval  WeightedCategoryCrossEntropy |  4.38028812\n",
            "Step    840: eval      WeightedCategoryAccuracy |  0.20238096\n",
            "\n",
            "Step    850: Ran 10 train steps in 39.02 secs\n",
            "Step    850: train WeightedCategoryCrossEntropy |  4.18496180\n",
            "Step    850: eval  WeightedCategoryCrossEntropy |  4.18386936\n",
            "Step    850: eval      WeightedCategoryAccuracy |  0.25016996\n",
            "\n",
            "Step    860: Ran 10 train steps in 39.68 secs\n",
            "Step    860: train WeightedCategoryCrossEntropy |  4.19933701\n",
            "Step    860: eval  WeightedCategoryCrossEntropy |  4.15598154\n",
            "Step    860: eval      WeightedCategoryAccuracy |  0.24084125\n",
            "\n",
            "Step    870: Ran 10 train steps in 38.57 secs\n",
            "Step    870: train WeightedCategoryCrossEntropy |  4.28249645\n",
            "Step    870: eval  WeightedCategoryCrossEntropy |  4.20228767\n",
            "Step    870: eval      WeightedCategoryAccuracy |  0.23917672\n",
            "\n",
            "Step    880: Ran 10 train steps in 38.27 secs\n",
            "Step    880: train WeightedCategoryCrossEntropy |  4.14814949\n",
            "Step    880: eval  WeightedCategoryCrossEntropy |  4.25458527\n",
            "Step    880: eval      WeightedCategoryAccuracy |  0.24606152\n",
            "\n",
            "Step    890: Ran 10 train steps in 39.53 secs\n",
            "Step    890: train WeightedCategoryCrossEntropy |  4.18638229\n",
            "Step    890: eval  WeightedCategoryCrossEntropy |  4.15930176\n",
            "Step    890: eval      WeightedCategoryAccuracy |  0.25366876\n",
            "\n",
            "Step    900: Ran 10 train steps in 38.60 secs\n",
            "Step    900: train WeightedCategoryCrossEntropy |  4.22820044\n",
            "Step    900: eval  WeightedCategoryCrossEntropy |  4.26476765\n",
            "Step    900: eval      WeightedCategoryAccuracy |  0.23472889\n",
            "\n",
            "Step    910: Ran 10 train steps in 38.33 secs\n",
            "Step    910: train WeightedCategoryCrossEntropy |  4.31323719\n",
            "Step    910: eval  WeightedCategoryCrossEntropy |  4.23973751\n",
            "Step    910: eval      WeightedCategoryAccuracy |  0.25162050\n",
            "\n",
            "Step    920: Ran 10 train steps in 38.08 secs\n",
            "Step    920: train WeightedCategoryCrossEntropy |  4.29046535\n",
            "Step    920: eval  WeightedCategoryCrossEntropy |  4.36382198\n",
            "Step    920: eval      WeightedCategoryAccuracy |  0.22352941\n",
            "\n",
            "Step    930: Ran 10 train steps in 39.45 secs\n",
            "Step    930: train WeightedCategoryCrossEntropy |  4.33584309\n",
            "Step    930: eval  WeightedCategoryCrossEntropy |  4.44683886\n",
            "Step    930: eval      WeightedCategoryAccuracy |  0.21767241\n",
            "\n",
            "Step    940: Ran 10 train steps in 38.66 secs\n",
            "Step    940: train WeightedCategoryCrossEntropy |  4.36007690\n",
            "Step    940: eval  WeightedCategoryCrossEntropy |  4.40013313\n",
            "Step    940: eval      WeightedCategoryAccuracy |  0.22389035\n",
            "\n",
            "Step    950: Ran 10 train steps in 38.84 secs\n",
            "Step    950: train WeightedCategoryCrossEntropy |  4.41642714\n",
            "Step    950: eval  WeightedCategoryCrossEntropy |  4.38631058\n",
            "Step    950: eval      WeightedCategoryAccuracy |  0.22022621\n",
            "\n",
            "Step    960: Ran 10 train steps in 39.87 secs\n",
            "Step    960: train WeightedCategoryCrossEntropy |  4.42081976\n",
            "Step    960: eval  WeightedCategoryCrossEntropy |  4.52037430\n",
            "Step    960: eval      WeightedCategoryAccuracy |  0.23013699\n",
            "\n",
            "Step    970: Ran 10 train steps in 38.73 secs\n",
            "Step    970: train WeightedCategoryCrossEntropy |  4.42298317\n",
            "Step    970: eval  WeightedCategoryCrossEntropy |  4.37332296\n",
            "Step    970: eval      WeightedCategoryAccuracy |  0.21446700\n",
            "\n",
            "Step    980: Ran 10 train steps in 38.60 secs\n",
            "Step    980: train WeightedCategoryCrossEntropy |  4.31715345\n",
            "Step    980: eval  WeightedCategoryCrossEntropy |  4.46902752\n",
            "Step    980: eval      WeightedCategoryAccuracy |  0.21929824\n",
            "\n",
            "Step    990: Ran 10 train steps in 39.79 secs\n",
            "Step    990: train WeightedCategoryCrossEntropy |  4.31248379\n",
            "Step    990: eval  WeightedCategoryCrossEntropy |  4.64227152\n",
            "Step    990: eval      WeightedCategoryAccuracy |  0.21477015\n",
            "\n",
            "Step   1000: Ran 10 train steps in 38.32 secs\n",
            "Step   1000: train WeightedCategoryCrossEntropy |  4.46292257\n",
            "Step   1000: eval  WeightedCategoryCrossEntropy |  4.41601133\n",
            "Step   1000: eval      WeightedCategoryAccuracy |  0.24282147\n",
            "\n",
            "Step   1010: Ran 10 train steps in 38.49 secs\n",
            "Step   1010: train WeightedCategoryCrossEntropy |  4.38810396\n",
            "Step   1010: eval  WeightedCategoryCrossEntropy |  4.37686729\n",
            "Step   1010: eval      WeightedCategoryAccuracy |  0.21701513\n",
            "\n",
            "Step   1020: Ran 10 train steps in 37.94 secs\n",
            "Step   1020: train WeightedCategoryCrossEntropy |  4.45975828\n",
            "Step   1020: eval  WeightedCategoryCrossEntropy |  4.54126024\n",
            "Step   1020: eval      WeightedCategoryAccuracy |  0.21783525\n",
            "\n",
            "Step   1030: Ran 10 train steps in 39.33 secs\n",
            "Step   1030: train WeightedCategoryCrossEntropy |  4.35161686\n",
            "Step   1030: eval  WeightedCategoryCrossEntropy |  4.45920897\n",
            "Step   1030: eval      WeightedCategoryAccuracy |  0.22770271\n",
            "\n",
            "Step   1040: Ran 10 train steps in 38.05 secs\n",
            "Step   1040: train WeightedCategoryCrossEntropy |  4.34907866\n",
            "Step   1040: eval  WeightedCategoryCrossEntropy |  4.23347569\n",
            "Step   1040: eval      WeightedCategoryAccuracy |  0.23254028\n",
            "\n",
            "Step   1050: Ran 10 train steps in 38.85 secs\n",
            "Step   1050: train WeightedCategoryCrossEntropy |  4.29744482\n",
            "Step   1050: eval  WeightedCategoryCrossEntropy |  4.32959509\n",
            "Step   1050: eval      WeightedCategoryAccuracy |  0.22835250\n",
            "\n",
            "Step   1060: Ran 10 train steps in 39.95 secs\n",
            "Step   1060: train WeightedCategoryCrossEntropy |  4.32458878\n",
            "Step   1060: eval  WeightedCategoryCrossEntropy |  4.31629896\n",
            "Step   1060: eval      WeightedCategoryAccuracy |  0.22878480\n",
            "\n",
            "Step   1070: Ran 10 train steps in 38.78 secs\n",
            "Step   1070: train WeightedCategoryCrossEntropy |  4.31470776\n",
            "Step   1070: eval  WeightedCategoryCrossEntropy |  4.05300236\n",
            "Step   1070: eval      WeightedCategoryAccuracy |  0.26023945\n",
            "\n",
            "Step   1080: Ran 10 train steps in 38.91 secs\n",
            "Step   1080: train WeightedCategoryCrossEntropy |  4.21481419\n",
            "Step   1080: eval  WeightedCategoryCrossEntropy |  4.17547274\n",
            "Step   1080: eval      WeightedCategoryAccuracy |  0.25180256\n",
            "\n",
            "Step   1090: Ran 10 train steps in 40.27 secs\n",
            "Step   1090: train WeightedCategoryCrossEntropy |  4.28666687\n",
            "Step   1090: eval  WeightedCategoryCrossEntropy |  4.37472248\n",
            "Step   1090: eval      WeightedCategoryAccuracy |  0.23757575\n",
            "\n",
            "Step   1100: Ran 10 train steps in 39.16 secs\n",
            "Step   1100: train WeightedCategoryCrossEntropy |  4.35466337\n",
            "Step   1100: eval  WeightedCategoryCrossEntropy |  4.40712833\n",
            "Step   1100: eval      WeightedCategoryAccuracy |  0.22066550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAGlkMZW6rKn"
      },
      "source": [
        "<a name=\"4\"></a>\n",
        "# 4. Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s48rWH-Pehpk"
      },
      "source": [
        "def attention(*args, **kwargs):\n",
        "    # number of input positions to remember in a cache when doing fast inference. \n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    # number of input elements to drop once the fast inference input cache fills up.\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    # return the attention layer with the parameters defined above\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# Getting the model with new attention for prediction\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si_Xo4nPehpk"
      },
      "source": [
        "# TRAX needs the model to be initialized with this shape\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "model.init(shape11)\n",
        "\n",
        "# Loading weights from the trained model\n",
        "model.weights = loop.eval_model.weights\n",
        "\n",
        "# saving the starting state for each new dialogue prediction\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdo9Fziy-8ZW",
        "outputId": "5f727ad7-323f-48b2-8f03-c5ea159ba172"
      },
      "source": [
        "str(model) == str(loop.eval_model)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB057xZMehpk"
      },
      "source": [
        "Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylnBKD1xehpk"
      },
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNw33qRBehpl"
      },
      "source": [
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    input_tokens_with_batch = input_tokens[None]\n",
        "    \n",
        "    # Using the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        model=ReformerLM,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd7Xr4Dmehpm"
      },
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    sentence = ''\n",
        "    counter = 0\n",
        "    \n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break    \n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNZNfs6Zehpm",
        "outputId": "5196ef8a-0101-41f5-be2a-4020cbde66a8"
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: : I have a train \n",
            "Person 1: I am the I need to like to? \n",
            "Person 2: I': I need to help. \n",
            "Person 2: I am a centre. \n",
            "Person 1: I need a train \n",
            "Person 1: I you like the centre. \n",
            "Person 1: I need to like to like range. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTXOnhJEehpm",
        "outputId": "27f7281b-d041-49cf-c9b6-1fa595aff4d9"
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Is there a hospital nearby?\n",
            "Person 1: : I am you like to the \n",
            "Person 1: I have you need to like a 4 2: I have a and I am you you I need the \n",
            "Person 1: I need the \n",
            "Person 2: I have the train \n",
            "Person 2: I you the centre. \n",
            "Person 2: I have. \n",
            "Person 2: I need to. \n",
            "Person 1: The address, you like to the reference 1 you like to book the reference. \n",
            "Person 1: I help. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wor46vEGehpm",
        "outputId": "aa48f2b0-4343-4096-ff90-74b373dd3b3a"
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 2: : I need a a a a a to like. \n",
            "Person 1: I need. \n",
            "Person 1: I am have 1: I need the \n",
            "Person 1: I am the \n",
            "Person 1: \n",
            "Person 1: I am like to like to like to the centre. \n",
            "Person 2: I need. \n",
            "Person 2: I have the \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}